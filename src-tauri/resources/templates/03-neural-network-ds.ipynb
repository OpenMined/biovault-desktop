{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header",
   "metadata": {},
   "source": [
    "# Beaver Tutorial 3: Neural Network Training (Data Scientist)\n",
    "\n",
    "Train a CNN on MNIST with privacy-preserving collaboration.\n",
    "\n",
    "Run this alongside `03-neural-network-do.ipynb` in a separate tab."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "setup-header",
   "metadata": {},
   "source": [
    "## Step 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "install",
   "metadata": {},
   "outputs": [],
   "source": [
    "!uv pip install torch torchvision matplotlib numpy -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Auto-load replies enabled for client2@sandbox.local (polling every 0.5s)\n",
      "üü¢ Active session loaded: d9efe11cc2d0\n",
      "   Peer: client1@sandbox.local\n",
      "  Deleted: 70f4d2314aa042f294288bdaeab645ef.beaver\n",
      "  Deleted: 11d8913689274cc9b52ccd9730dae4da.beaver\n",
      "  Deleted: data/ (0 files)\n",
      "‚úì Session reset: 2 files deleted\n",
      "You: client2@sandbox.local\n",
      "Peer: client1@sandbox.local\n"
     ]
    }
   ],
   "source": [
    "import beaver\n",
    "from beaver import Twin\n",
    "import time\n",
    "\n",
    "bv = beaver.ctx()\n",
    "session = bv.active_session()\n",
    "session.reset(force=True)\n",
    "\n",
    "print(f\"You: {bv.user}\")\n",
    "print(f\"Peer: {session.peer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e801fbaa-3403-4b9b-bd12-95e1ca5a3eaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# session.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wait-data-header",
   "metadata": {},
   "source": [
    "## Step 2: Wait for MNIST Data\n",
    "\n",
    "**Run DO notebook Steps 1-3 first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-data",
   "metadata": {},
   "outputs": [],
   "source": "mnist = session.wait_for_remote_var(\"mnist\", timeout=120, trust_loader=True)\nmnist"
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "preview",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mock: X_train (500, 28, 28), y_train (500,)\n",
      "Classes: 10\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mock = mnist.public\n",
    "X_train = np.array(mock['X_train'])\n",
    "y_train = np.array(mock['y_train'])\n",
    "print(f\"Mock: X_train {X_train.shape}, y_train {y_train.shape}\")\n",
    "print(f\"Classes: {mock['num_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "model-header",
   "metadata": {},
   "source": [
    "## Step 3: Define CNN Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "define-training",
   "metadata": {},
   "outputs": [],
   "source": "@bv\ndef train_cnn(data: dict, on_progress=None) -> dict:\n    \"\"\"Train a CNN on MNIST data with optional progress callback.\"\"\"\n    import torch\n    import torch.nn as nn\n    import torch.optim as optim\n    from torch.utils.data import DataLoader, TensorDataset\n    import numpy as np\n    import matplotlib.pyplot as plt\n    \n    print(\"=\"*50)\n    print(\"CNN Training\")\n    print(\"=\"*50)\n    \n    # Convert lists to numpy then to tensors\n    X_train = torch.tensor(np.array(data['X_train']), dtype=torch.float32).unsqueeze(1)\n    y_train = torch.tensor(np.array(data['y_train']), dtype=torch.long)\n    X_test = torch.tensor(np.array(data['X_test']), dtype=torch.float32).unsqueeze(1)\n    y_test = torch.tensor(np.array(data['y_test']), dtype=torch.long)\n    \n    print(f\"Train: {len(X_train)}, Test: {len(X_test)}\")\n    \n    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=64, shuffle=True)\n    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=64)\n    \n    # Simple CNN\n    class CNN(nn.Module):\n        def __init__(self):\n            super().__init__()\n            self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n            self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n            self.pool = nn.MaxPool2d(2, 2)\n            self.fc1 = nn.Linear(64 * 7 * 7, 128)\n            self.fc2 = nn.Linear(128, 10)\n            self.relu = nn.ReLU()\n            self.dropout = nn.Dropout(0.25)\n            \n        def forward(self, x):\n            x = self.pool(self.relu(self.conv1(x)))\n            x = self.pool(self.relu(self.conv2(x)))\n            x = x.view(-1, 64 * 7 * 7)\n            x = self.dropout(self.relu(self.fc1(x)))\n            return self.fc2(x)\n    \n    model = CNN()\n    criterion = nn.CrossEntropyLoss()\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    \n    history = {'loss': [], 'train_acc': [], 'test_acc': []}\n    total_epochs = 5\n    \n    for epoch in range(total_epochs):\n        model.train()\n        total_loss, correct, total = 0, 0, 0\n        \n        for inputs, labels in train_loader:\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            total_loss += loss.item()\n            _, pred = outputs.max(1)\n            total += labels.size(0)\n            correct += pred.eq(labels).sum().item()\n        \n        train_acc = 100. * correct / total\n        avg_loss = total_loss / len(train_loader)\n        \n        # Test\n        model.eval()\n        test_correct, test_total = 0, 0\n        with torch.no_grad():\n            for inputs, labels in test_loader:\n                outputs = model(inputs)\n                _, pred = outputs.max(1)\n                test_total += labels.size(0)\n                test_correct += pred.eq(labels).sum().item()\n        test_acc = 100. * test_correct / test_total\n        \n        history['loss'].append(avg_loss)\n        history['train_acc'].append(train_acc)\n        history['test_acc'].append(test_acc)\n        \n        print(f\"Epoch {epoch+1}/{total_epochs}: train_acc={train_acc:.2f}%, test_acc={test_acc:.2f}%\")\n        \n        # Call progress callback if provided (resolves to DO's local function!)\n        if on_progress and callable(on_progress):\n            try:\n                on_progress(epoch + 1, total_epochs, train_acc, test_acc)\n            except Exception as e:\n                print(f\"Progress callback error: {e}\")\n    \n    # Plot\n    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 4))\n    ax1.plot(history['loss'])\n    ax1.set_title('Loss')\n    ax1.set_xlabel('Epoch')\n    ax2.plot(history['train_acc'], label='Train')\n    ax2.plot(history['test_acc'], label='Test')\n    ax2.set_title('Accuracy')\n    ax2.set_xlabel('Epoch')\n    ax2.legend()\n    plt.tight_layout()\n    plt.show()\n    \n    print(f\"\\nFinal Test Accuracy: {test_acc:.2f}%\")\n    \n    # Save model weights with safetensors\n    from safetensors.torch import save_model\n    import tempfile\n    import os\n    \n    with tempfile.NamedTemporaryFile(suffix='.safetensors', delete=False) as f:\n        model_path = f.name\n    save_model(model, model_path)\n    with open(model_path, 'rb') as f:\n        model_bytes = f.read()\n    os.unlink(model_path)\n    \n    print(f\"Model saved: {len(model_bytes)} bytes\")\n    \n    return {\n        'test_accuracy': test_acc,\n        'train_accuracy': train_acc,\n        'final_loss': history['loss'][-1],\n        'history': history,\n        'model_weights': model_bytes,\n    }"
  },
  {
   "cell_type": "markdown",
   "id": "test-header",
   "metadata": {},
   "source": [
    "## Step 4: Test on Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "test-local",
   "metadata": {},
   "outputs": [],
   "source": "# Load the callback from DO (for passing to remote execution)\nif \"on_progress\" in session.peer_remote_vars:\n    on_progress = session.peer_remote_vars[\"on_progress\"].load(inject=False, trust_loader=True)\n    print(f\"Callback loaded: {on_progress}\")\nelse:\n    on_progress = None\n    print(\"No callback available (will work without live progress)\")\n\n# Test locally on mock data (callback won't do anything useful here - no DO context)\nresult = train_cnn(mnist)\nprint(f\"\\nMock accuracy: {result.public['test_accuracy']:.2f}%\")\nresult.show_figures(\"public\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "request-private",
   "metadata": {},
   "outputs": [],
   "source": "# Request private execution, passing the callback\n# When DO runs this, on_progress resolves to THEIR local function!\nresult_with_callback = train_cnn(mnist, on_progress=on_progress)\nresult_with_callback.request_private()\nprint(\"Request sent with callback! Run DO notebook Steps 5-8...\")"
  },
  {
   "cell_type": "markdown",
   "id": "wait-header",
   "metadata": {},
   "source": [
    "## Step 5: Monitor Progress & Receive Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monitor",
   "metadata": {},
   "outputs": [],
   "source": "# Monitor training progress (using LiveVar now, simpler!)\nprogress = session.wait_for_remote_var(\"progress\", timeout=60, trust_loader=True)\nif progress:\n    print(\"Monitoring training progress...\")\n    print(\"-\" * 40)\n    last_epoch = -1\n    for _ in range(120):\n        progress = session.peer_remote_vars[\"progress\"].load(inject=False, auto_accept=True, trust_loader=True)\n        p = progress.value if hasattr(progress, 'value') else progress.public\n        status = p.get('status', '')\n        epoch = p.get('epoch', 0)\n        \n        if epoch != last_epoch or status == 'complete':\n            if status == 'waiting':\n                print(\"‚è≥ Waiting for training to start...\")\n            elif status in ('starting', 'training'):\n                train_acc = p.get('train_acc', 0)\n                test_acc = p.get('test_acc', 0)\n                if train_acc:\n                    print(f\"üöÄ Epoch {epoch}/{p.get('total_epochs', 5)}: train={train_acc:.1f}%, test={test_acc:.1f}%\")\n                else:\n                    print(f\"üöÄ Training started...\")\n            elif status == 'complete':\n                print(f\"‚úÖ Complete! Train={p.get('train_acc', 0):.1f}%, Test={p.get('test_acc', 0):.1f}%\")\n                break\n            last_epoch = epoch\n        \n        time.sleep(0.5)\n    print(\"-\" * 40)"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91d78b5e-6aa2-441f-82d1-43f9f4ad7df6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b>InboxView(/Users/madhavajay/dev/biovault-desktop/workspace2/biovault/sandbox/client2@sandbox.local/datasites/client1@sandbox.local/shared/biovault/sessions/d9efe11cc2d0): empty</b>"
      ],
      "text/plain": [
       "InboxView(/Users/madhavajay/dev/biovault-desktop/workspace2/biovault/sandbox/client2@sandbox.local/datasites/client1@sandbox.local/shared/biovault/sessions/d9efe11cc2d0): empty"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "session.inbox()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wait-results",
   "metadata": {},
   "outputs": [],
   "source": "approved = bv.wait_for_response(result_with_callback, timeout=600)\nprint(f\"\\nReal Test Accuracy: {approved.private['test_accuracy']:.2f}%\")\napproved.show_figures(\"private\")"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Mock vs Real ===\n",
      "Mock: 85.00%\n",
      "Real: 97.30%\n",
      "\n",
      "Model received: 1687168 bytes\n"
     ]
    }
   ],
   "source": [
    "# Compare mock vs real results\n",
    "print(\"=== Mock vs Real ===\")\n",
    "print(f\"Mock: {result.public['test_accuracy']:.2f}%\")\n",
    "print(f\"Real: {approved.private['test_accuracy']:.2f}%\")\n",
    "print(f\"\\nModel received: {len(approved.private.get('model_weights', b''))} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9pngbxxpsdg",
   "metadata": {},
   "source": [
    "## Step 6: Load and Test the Trained Model\n",
    "\n",
    "The DO sent us the trained model weights. Let's load and verify it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "lyawh4zuknf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Model loaded: 421642 parameters\n"
     ]
    }
   ],
   "source": [
    "# Load the trained model from safetensors bytes\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from safetensors.torch import load_model\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "# Define the same CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 7 * 7)\n",
    "        x = self.dropout(self.relu(self.fc1(x)))\n",
    "        return self.fc2(x)\n",
    "\n",
    "# Load model weights\n",
    "model_bytes = approved.private.get('model_weights', b'')\n",
    "if model_bytes:\n",
    "    # Write bytes to temp file and load\n",
    "    with tempfile.NamedTemporaryFile(suffix='.safetensors', delete=False) as f:\n",
    "        f.write(model_bytes)\n",
    "        temp_path = f.name\n",
    "    \n",
    "    trained_model = CNN()\n",
    "    load_model(trained_model, temp_path)\n",
    "    os.unlink(temp_path)\n",
    "    \n",
    "    trained_model.eval()\n",
    "    print(f\"‚úì Model loaded: {sum(p.numel() for p in trained_model.parameters())} parameters\")\n",
    "else:\n",
    "    print(\"No model weights received\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "zhj0i83w8ur",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model verified on local mock data: 99.00%\n",
      "\n",
      "üéâ Success! We have a trained model without ever seeing the real data!\n"
     ]
    }
   ],
   "source": [
    "# Test the model on our mock data to verify it works\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Get our mock test data\n",
    "mock_X_test = torch.tensor(np.array(mnist.public['X_test']), dtype=torch.float32).unsqueeze(1)\n",
    "mock_y_test = torch.tensor(np.array(mnist.public['y_test']), dtype=torch.long)\n",
    "\n",
    "test_loader = DataLoader(TensorDataset(mock_X_test, mock_y_test), batch_size=32)\n",
    "\n",
    "# Run inference\n",
    "trained_model.eval()\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = trained_model(inputs)\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += labels.size(0)\n",
    "        correct += predicted.eq(labels).sum().item()\n",
    "\n",
    "local_accuracy = 100. * correct / total\n",
    "print(f\"‚úÖ Model verified on local mock data: {local_accuracy:.2f}%\")\n",
    "print(f\"\\nüéâ Success! We have a trained model without ever seeing the real data!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "summary",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this tutorial you:\n",
    "\n",
    "1. **Defined CNN architecture** - Same model runs on both mock and real data\n",
    "2. **Tested on mock data** - Verified function works before sending to DO\n",
    "3. **Monitored live progress** - Watched epoch-by-epoch accuracy in real-time\n",
    "4. **Received trained model** - Got model weights via safetensors\n",
    "5. **Verified locally** - Ran inference on mock data to confirm model works\n",
    "\n",
    "### Privacy Preserved!\n",
    "\n",
    "- You received training curves and accuracy metrics\n",
    "- You received the **trained model weights**\n",
    "- You did **NOT** receive the raw MNIST images\n",
    "- DO controlled what was shared at every step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7927227-23aa-4f65-ac2b-60474fd3c954",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcac9b9c-321c-4d1c-971b-7d8d79e9fa1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9754c6d6-19fc-4392-8d96-c0bc9213d0c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}